{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5332886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562794a",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55dcf2",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "    - Create Session\n",
    "    - Read from CSV\n",
    "    - Show Schema from DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37190816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bec12f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14d51aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "df = spark.read.option(\"delimiter\",\";\").option(\"header\",\"true\").csv('example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f6aaa19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| Nama|Umur|\n",
      "+-----+----+\n",
      "|Hanif|  22|\n",
      "|Ahmad|  18|\n",
      "| Dudu|  23|\n",
      "| Bubu|  23|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36189068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb16ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Nama='Hanif', Umur='22')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a25b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nama: string (nullable = true)\n",
      " |-- Umur: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96b595",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "    - Describe\n",
    "    - Add and Drop Columns\n",
    "    - Rename Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b354dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset with custom format columns\n",
    "df1 = spark.read.option('delimiter',';').option('header','true').csv('example1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f824ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+\n",
      "| Nama|Umur|Pengalaman|\n",
      "+-----+----+----------+\n",
      "|Hanif|  23|         2|\n",
      "| Dika|  30|         8|\n",
      "| Tino|  32|         5|\n",
      "| Muiz|  28|         7|\n",
      "+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01529d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nama: string (nullable = true)\n",
      " |-- Umur: integer (nullable = true)\n",
      " |-- Pengalaman: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "983f47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.option('delimiter',';').csv('example1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ceecfdb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nama: string (nullable = true)\n",
      " |-- Umur: integer (nullable = true)\n",
      " |-- Pengalaman: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a8d8a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nama', 'Umur', 'Pengalaman']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecf70091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| Nama|Umur|\n",
      "+-----+----+\n",
      "|Hanif|  23|\n",
      "| Dika|  30|\n",
      "| Tino|  32|\n",
      "| Muiz|  28|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('Nama','Umur').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a33a5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+------------------+\n",
      "|summary|Nama|             Umur|        Pengalaman|\n",
      "+-------+----+-----------------+------------------+\n",
      "|  count|   4|                4|                 4|\n",
      "|   mean|null|            28.25|               5.5|\n",
      "| stddev|null|3.862210075418823|2.6457513110645907|\n",
      "|    min|Dika|               23|                 2|\n",
      "|    max|Tino|               32|                 8|\n",
      "+-------+----+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.describe().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1390343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+-----------------------+\n",
      "| Nama|Umur|Pengalaman|Experience After 2 year|\n",
      "+-----+----+----------+-----------------------+\n",
      "|Hanif|  23|         2|                      4|\n",
      "| Dika|  30|         8|                     10|\n",
      "| Tino|  32|         5|                      7|\n",
      "| Muiz|  28|         7|                      9|\n",
      "+-----+----+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add columns\n",
    "df1 = df1.withColumn('Experience After 2 year',df1['Pengalaman']+2)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b72f682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+\n",
      "| Nama|Umur|Pengalaman|\n",
      "+-----+----+----------+\n",
      "|Hanif|  23|         2|\n",
      "| Dika|  30|         8|\n",
      "| Tino|  32|         5|\n",
      "| Muiz|  28|         7|\n",
      "+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns\n",
    "df1 = df1.drop('Experience After 2 year')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad504faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+\n",
      "|Nama Baru|Umur|Pengalaman|\n",
      "+---------+----+----------+\n",
      "|    Hanif|  23|         2|\n",
      "|     Dika|  30|         8|\n",
      "|     Tino|  32|         5|\n",
      "|     Muiz|  28|         7|\n",
      "+---------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename column\n",
    "df1 = df1.withColumnRenamed('Nama','Nama Baru')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abdc8f",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "    - Drop Rows\n",
    "    - Parameter to Drop\n",
    "    - Handling Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ac9581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('test2.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd463e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop null value\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1d3449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop null value with thresh\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ef35733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null value with subset\n",
    "df.na.drop(subset='Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b09ce311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "| John Doe|  34|        10| 38000|\n",
      "| John Doe|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill null value\n",
    "df.na.fill('John Doe',subset='Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "993bc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute with new columns\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd3369a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|null|      null| 40000|         29|                 4|         40000|\n",
      "|     null|  34|        10| 38000|         34|                10|         38000|\n",
      "|     null|  36|      null|  null|         36|                 4|         20000|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62e009",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "    \n",
    "    - Filter Operation\n",
    "    - &|==\n",
    "    - ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "db8c9bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|  Sunny|\n",
      "|   Paul|\n",
      "| Harsha|\n",
      "|Shubham|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Who People have Salary less than or same than 20.000\n",
    "df.filter('Salary<=20000').select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4dd204ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternative\n",
    "df.filter(df['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f65b91dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Who People have Salary less than 20.000 and greater than 15.000\n",
    "df.filter((df['Salary']<20000)&(df['Salary']>15000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f25c8",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "\n",
    "    - Groupby and aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "346afc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('test3.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35f2185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(Salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby operation\n",
    "df.groupBy('Name').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5283aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|max(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      10000|\n",
      "|    Big Data|       5000|\n",
      "|Data Science|      20000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by departement which gives maximum salary\n",
    "df.groupBy('Departments').max('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d1a934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e330387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+-------------+\n",
      "| Departments|sum(Salary)|avg(Salary)|count(salary)|\n",
      "+------------+-----------+-----------+-------------+\n",
      "|         IOT|      15000|     7500.0|            2|\n",
      "|    Big Data|      15000|     3750.0|            4|\n",
      "|Data Science|      43000|    10750.0|            4|\n",
      "+------------+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').agg(F.sum('Salary'),F.mean('Salary'),F.count('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612beca",
   "metadata": {},
   "source": [
    "## Part 6\n",
    "\n",
    "    - MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1dd9b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6d559fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7c34d974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read The dataset\n",
    "training = spark.read.csv('test2.csv',header=True,inferSchema=True)\n",
    "training = training.na.drop()\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "59f26269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "878151d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8ede1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"age\",\"Experience\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4b03d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1f1b30a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary', 'Independent Features']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e38db971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------------+\n",
      "|     Name|age|Experience|Salary|Independent Features|\n",
      "+---------+---+----------+------+--------------------+\n",
      "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f256181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = output.select('Independent Features','Salary')\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "859b6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train,test = final.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol='Independent Features',labelCol='Salary')\n",
    "regressor = regressor.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "119e1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent Features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [21.0,1.0]| 15000|17429.234338747105|\n",
      "|         [31.0,10.0]| 30000| 27547.56380510443|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = regressor.evaluate(test)\n",
    "pred.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f7e4f1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2440.8352668213374, 5957811.381290973)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.meanAbsoluteError,pred.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
